{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BraTS Dataset Evaluation Script\n",
    "\n",
    "This script loads a trained A4-Unet model and evaluates it on the BraTS dataset,\n",
    "providing visualization capabilities for the results.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display\n",
    "from evaluate import evaluate\n",
    "from collections import OrderedDict\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from a4unet.dataloader.bratsloader import BRATSDataset3D\n",
    "from a4unet.a4unet import create_a4unet_model\n",
    "\n",
    "\n",
    "def setup_paths():\n",
    "    \"\"\"Define and return paths used in the script.\"\"\"\n",
    "    return {\n",
    "        'brats_dir': 'C:/Users/admin/Desktop/Spring2025/AIPI540/CV/MICCAI_BraTS2020_TrainingData',\n",
    "        'val_dir': 'testset',\n",
    "        'output_dir': 'C:/Users/admin/Desktop/Spring2025/AIPI540/CV/A4-Unet/evaluate_result',\n",
    "        'checkpoint_path': 'checkpoints/checkpoint_epoch15.pth'\n",
    "    }\n",
    "\n",
    "\n",
    "def initialize_model(checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Initialize the A4-Unet model and load checkpoints.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the model checkpoint\n",
    "        device (torch.device): Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Module: Initialized model\n",
    "    \"\"\"\n",
    "    model = create_a4unet_model(\n",
    "        image_size=128,\n",
    "        num_channels=128,\n",
    "        num_res_blocks=2,\n",
    "        num_classes=2,\n",
    "        learn_sigma=True,\n",
    "        in_ch=4\n",
    "    )\n",
    "    \n",
    "    state_dict = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "    new_state_dict = OrderedDict(\n",
    "        (k, v) for k, v in state_dict.items() if k != 'mask_values'\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_dataset(val_dir):\n",
    "    \"\"\"\n",
    "    Prepare the BraTS dataset with appropriate transformations.\n",
    "    \n",
    "    Args:\n",
    "        val_dir (str): Directory containing validation data\n",
    "        \n",
    "    Returns:\n",
    "        BRATSDataset3D: Prepared dataset\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128), antialias=True)\n",
    "    ])\n",
    "    return BRATSDataset3D(val_dir, transform=transform, test_flag=False)\n",
    "\n",
    "\n",
    "def predict_masks(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Generate predictions for all images in the dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model\n",
    "        dataloader (BRATSDataset3D): Dataset loader\n",
    "        device (torch.device): Device to perform computations on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Lists containing images, true masks, and predicted masks\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    true_masks = []\n",
    "    pred_masks = []\n",
    "    \n",
    "    for slice_data in tqdm(dataloader, desc='Predict masks', unit='slice', leave=False):\n",
    "        image, mask, _ = slice_data\n",
    "        image = image.unsqueeze(0).to(\n",
    "            device=device,\n",
    "            dtype=torch.float32,\n",
    "            memory_format=torch.channels_last\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_mask = model(image)\n",
    "        \n",
    "        image = image.squeeze(0).cpu().numpy()\n",
    "        pred_mask = pred_mask.argmax(dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "        images.append(image)\n",
    "        true_masks.append(mask)\n",
    "        pred_masks.append(pred_mask)\n",
    "    \n",
    "    return images, true_masks, pred_masks\n",
    "\n",
    "\n",
    "def plot_image_and_mask_with_slider(images, masks_true, masks_pred=None, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Create an interactive plot with a slider to view different slices.\n",
    "    \n",
    "    Args:\n",
    "        images (list): List of image arrays\n",
    "        masks_true (list): List of true mask arrays\n",
    "        masks_pred (list, optional): List of predicted mask arrays\n",
    "        figsize (tuple): Figure size (width, height)\n",
    "    \"\"\"\n",
    "    def plot_slice(slice_idx):\n",
    "        plt.figure(figsize=figsize)\n",
    "        modality = ['T1', 'T1ce', 'T2', 'FLAIR']\n",
    "        \n",
    "        # Plot modalities\n",
    "        for i, mod in enumerate(modality):\n",
    "            plt.subplot(2, 4, i + 1)\n",
    "            plt.imshow(np.squeeze(images[slice_idx][i]), cmap='gray')\n",
    "            plt.title(mod)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Plot masks\n",
    "        plt.subplot(2, 4, 6)\n",
    "        plt.imshow(np.squeeze(masks_true[slice_idx]), cmap='gray')\n",
    "        plt.title('True Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if masks_pred is not None:\n",
    "            plt.subplot(2, 4, 7)\n",
    "            plt.imshow(np.squeeze(masks_pred[slice_idx]), cmap='gray')\n",
    "            plt.title('Predicted Mask')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    slice_slider = widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=len(images) - 1,\n",
    "        step=1,\n",
    "        description='Slice'\n",
    "    )\n",
    "    interactive_plot = widgets.interactive(plot_slice, slice_idx=slice_slider)\n",
    "    display(interactive_plot)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Setup\n",
    "    paths = setup_paths()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize model and dataset\n",
    "    model = initialize_model(paths['checkpoint_path'], device)\n",
    "    dataset = prepare_dataset(paths['val_dir'])\n",
    "    \n",
    "    # Generate predictions\n",
    "    images, true_masks, pred_masks = predict_masks(model, dataset, device)\n",
    "    \n",
    "    # Display results\n",
    "    plot_image_and_mask_with_slider(images, true_masks, pred_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict masks:   0%|          | 0/155 [00:00<?, ?slice/s]c:\\Users\\admin\\anaconda3\\envs\\a4unet\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "                                                                   \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93b638f05984cbf92f959b5475e1e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Slice', max=154), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "# from utils.dice_score import multiclass_dice_coeff\n",
    "\n",
    "# mask_true = mask_true.to(device=device, dtype=torch.long)\n",
    "# mask_true = F.one_hot(mask_true, model.n_classes).permute(0, 3, 1, 2).float()\n",
    "# mask_pred = F.one_hot(mask_pred.argmax(dim=1), model.n_classes).permute(0, 3, 1, 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass_dice_coeff(mask_pred, mask_true, reduce_batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import jaccard_score\n",
    "# jaccard_score(mask_true.argmax(dim=1).cpu().numpy().flatten(),\n",
    "#             mask_pred.argmax(dim=1).cpu().numpy().flatten(), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import directed_hausdorff\n",
    "# np.percentile(directed_hausdorff(mask_pred.cpu().numpy().reshape(1, -1), mask_true.cpu().numpy().reshape(1, -1))[0], 95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a4unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
